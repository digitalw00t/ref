#!/usr/bin/env python3
# filename: ref
# author: draeician (July 22, 2023)
# purpose: to allow for fast CLI recording from the command line for later reference

import os
import re
import sys
import requests
import argparse
import warnings
import logging
from datetime import datetime
from urllib.parse import urlparse, parse_qs
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from bs4 import BeautifulSoup
from dotenv import load_dotenv, set_key
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# Library versions
import urllib3
import chardet
import charset_normalizer

urllib3.__version__ = '2.0.4'
chardet.__version__ = '5.2.0'
charset_normalizer.__version__ = '3.2.0'

__VERSION__ = "v1.4.1"

# Filter out the RequestsDependencyWarning
warnings.filterwarnings("ignore", category=DeprecationWarning, module="urllib3")

# Set environment path
env_path = os.path.join(os.path.expanduser("~"), '.env')
load_dotenv(dotenv_path=env_path)

# YouTube API details
YOUTUBE_API_SERVICE_NAME = 'youtube'
YOUTUBE_API_VERSION = 'v3'
DEVELOPER_KEY = os.getenv('YOUTUBE_API_KEY')

# Base path for references
BASE = os.path.expanduser("~/")
UNIFIED = os.path.join(BASE, "references", "references.md")

# Logging configuration
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s:%(message)s')

def resolve_redirect(url: str) -> str:
    """
    Resolves the final URL after following any redirects. Specifically handles YouTube redirect URLs.

    Args:
        url (str): The original URL to resolve.

    Returns:
        str: The final URL after following redirects.
    """
    youtube_redirect_pattern = re.compile(r'https://www\.youtube\.com/redirect\?')
    if youtube_redirect_pattern.match(url):
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)
        if 'q' in query_params:
            return query_params['q'][0]
    
    session = requests.Session()
    try:
        response = session.head(url, allow_redirects=True)
        return response.url
    except requests.exceptions.RequestException as e:
        logging.error(f"Error resolving redirect for URL: {url}, error: {e}")
        return url

def check_integrity():
    """
    Checks the integrity of the 'references.md' file to ensure that each line follows the expected format.
    
    Returns:
        list: A list of tuples containing details of lines that do not match the expected format.
    """
    errors = []
    with open(UNIFIED, "r") as file:
        for line_number, line in enumerate(file, start=1):
            if not re.match(r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\|\[.*\]\(.*\)\|\(.*\)\|.*\|(YouTube|General)\n$', line):
                expected_line = f'{datetime.now().isoformat()}|[URL]|(Title)|Source|(YouTube|General)'
                errors.append((f"references.md", line_number, line.strip(), expected_line))
    return errors

def set_developer_key():
    """
    Prompts the user to enter their YouTube API key and sets it in the environment variables.
    """
    key = input("Please enter your YOUTUBE_API_KEY: ")
    os.environ['YOUTUBE_API_KEY'] = key
    set_key(env_path, 'YOUTUBE_API_KEY', key)
    print("YOUTUBE_API_KEY set successfully!")

def get_youtube_data(url: str) -> tuple:
    """
    Fetches YouTube video or playlist data using the YouTube Data API.

    Args:
        url (str): The YouTube URL to fetch data for.

    Returns:
        tuple: Video ID, title, and channel title for a single video.
        tuple: Playlist title, uploader, and list of video details for a playlist.

    Raises:
        ValueError: If the YouTube URL is invalid.
    """
    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)
    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)

    if 'list' in query_params:
        return get_youtube_playlist_data(query_params['list'][0], youtube)

    video_id = query_params.get('v')
    if not video_id:
        # Check if it's a Shorts URL
        shorts_match = re.match(r'/shorts/([^/?]+)', parsed_url.path)
        if shorts_match:
            video_id = shorts_match.group(1)
        else:
            raise ValueError("Invalid YouTube URL")

    video_id = video_id[0]  # Extract video ID from the list
    video_response = youtube.videos().list(part='snippet', id=video_id).execute()
    video_data = video_response['items'][0]['snippet']
    return video_id, video_data['title'], video_data['channelTitle']

def get_youtube_playlist_data(playlist_id: str, youtube) -> tuple:
    """
    Fetches YouTube playlist data using the YouTube Data API.

    Args:
        playlist_id (str): The YouTube playlist ID.
        youtube: The YouTube API client.

    Returns:
        tuple: A tuple containing the playlist title, uploader, and a list of tuples for each video in the playlist.
               Each video tuple contains video ID, title, and uploader.
    """
    # Get playlist metadata
    playlist_response = youtube.playlists().list(part='snippet', id=playlist_id).execute()
    if not playlist_response['items']:
        raise ValueError("Invalid YouTube Playlist ID")

    playlist_snippet = playlist_response['items'][0]['snippet']
    playlist_title = playlist_snippet['title']
    playlist_uploader = playlist_snippet['channelTitle']

    # Get videos in the playlist
    video_details = []
    next_page_token = None
    while True:
        playlist_items_response = youtube.playlistItems().list(
            part='snippet',
            maxResults=50,
            playlistId=playlist_id,
            pageToken=next_page_token
        ).execute()
        for item in playlist_items_response['items']:
            video_id = item['snippet']['resourceId']['videoId']
            title = item['snippet']['title']
            uploader = item['snippet']['channelTitle']
            video_details.append((video_id, title, uploader))
        next_page_token = playlist_items_response.get('nextPageToken')
        if not next_page_token:
            break

    return playlist_title, playlist_uploader, video_details

def ensure_path_exists(file_path: str):
    """
    Ensures that the directory and file specified by `file_path` exist. Creates them if they do not exist.
    
    Args:
        file_path (str): The path to the file to ensure existence.
    """
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)
    if not os.path.exists(file_path):
        open(file_path, 'w').close()

def append_to_file(file_path: str, line: str) -> None:
    """
    Appends a line to the specified file, ensuring the path exists.
    
    Args:
        file_path (str): The path to the file.
        line (str): The line to append to the file.
    """
    ensure_path_exists(file_path)
    with open(file_path, "a") as f:
        f.write(line)
        f.flush()
        os.fsync(f.fileno())

def search_entries(search_term: str, search_field: str, file_path: str) -> dict:
    """
    Searches for entries in a file based on a specified search term and field.

    Args:
        search_term (str): The term to search for within the specified field.
        search_field (str): The field to search within. Valid options are "url", "title", "date", "source", and "uploader".
        file_path (str): The path to the file where the search will be conducted.

    Returns:
        dict: A dictionary where keys are lines from the file that match the search criteria and values are lists of hit types.
    """
    results = {}
    search_term_lower = search_term.lower()  # Convert search term to lowercase for case-insensitive search

    with open(file_path, "r") as file:
        for line in file:
            fields = line.split('|')
            if len(fields) < 5:
                logging.warning(f"Line does not have the expected number of fields: {line.strip()}")
                continue

            hit_types = []
            # Perform case-insensitive search
            if search_field == "url" and search_term_lower in fields[1].lower():
                hit_types.append("Url")
            elif search_field == "title" and search_term_lower in fields[2].lower():
                hit_types.append("Title")
            elif search_field == "date" and search_term_lower in fields[0].lower():
                hit_types.append("Date")
            elif search_field == "source" and search_term_lower in fields[4].lower():
                hit_types.append("Source")
            elif search_field == "uploader" and search_term_lower in fields[3].lower():
                hit_types.append("Uploader")

            if hit_types:
                if line not in results:
                    results[line] = hit_types
                else:
                    results[line].extend(hit_types)

    return results

def parse_arguments() -> argparse.Namespace:
    """
    Parses command-line arguments and returns the parsed arguments as a Namespace object.

    Returns:
        argparse.Namespace: The parsed command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Add or search URL entries in markdown files.")
    parser.add_argument("url", nargs='?', default=None, help="URL to be added.")
    parser.add_argument("-f", "--force", action="store_true", help="Force addition even if URL already exists.")
    parser.add_argument("-e", "--edit", action="store_true", help="Open markdown file for editing.")
    parser.add_argument("-d", "--debug", type=int, choices=[1, 2, 3], help="Set the debug level: 1 for INFO, 2 for WARNING, 3 for DEBUG.")
    parser.add_argument("--integrity", action="store_true", help="Check the integrity of log files.")
    parser.add_argument("-b", "--backup", action="store_true", help="Create a backup of the references.md file.")
    parser.add_argument("--search-url", help="Search entries by URL.")
    parser.add_argument("--search-title", help="Search entries by title.")
    parser.add_argument("--search-date", help="Search entries by date.")
    parser.add_argument("--search-source", help="Search entries by source.")
    parser.add_argument("--search-uploader", help="Search entries by uploader.")
    parser.add_argument("--search", help="Search entries across all fields (URL, title, date, source, uploader).")
    parser.add_argument("--version", action="version", version=f"%(prog)s {__VERSION__}")
    args = parser.parse_args()
    if args.edit:
        os.system(f"vim {UNIFIED}")
        sys.exit()
    if args.url:
        args.url = args.url.replace('&', '\\&')
    return args

def url_exists_in_file(url: str, file_path: str) -> bool:
    """
    Checks if a URL already exists in the specified file.
    
    Args:
        url (str): The URL to check.
        file_path (str): The path to the file.
    
    Returns:
        bool: True if the URL exists in the file, False otherwise.
    """
    with open(file_path, "r") as f:
        for line in f:
            # Extract the URL part from the line and compare exactly
            match = re.search(r'\[([^\]]+)\]', line)
            if match and match.group(1) == url:
                return True
    return False

def get_title_from_url(url: str) -> str:
    """
    Fetches the title of a webpage given its URL.
    
    Args:
        url (str): The URL of the webpage.
    
    Returns:
        str: The title of the webpage, or an error message if the title cannot be fetched.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
    }
    session = requests.Session()
    retry = Retry(
        total=10,  # Increased total retries
        read=10,
        connect=10,
        backoff_factor=1,  # Increased backoff factor
        status_forcelist=(500, 502, 503, 504),  # Include 503 in status_forcelist
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    try:
        response = session.get(url, headers=headers, timeout=30)  # Timeout remains at 30 seconds
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        title = soup.title.string.strip() if soup.title and soup.title.string else "No title found"
        return title
    except requests.exceptions.Timeout:
        logging.error(f"Timeout error fetching title: {url}")
        return "Timeout error"
    except requests.exceptions.TooManyRedirects:
        logging.error(f"Too many redirects: {url}")
        return "Too many redirects"
    except requests.exceptions.RequestException as e:
        logging.error(f"Error fetching title: {e}")
        return "Dead link"
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}")
        return "Unexpected error"

def process_url(url: str, force: bool) -> None:
    """
    Processes a given URL to extract and record relevant information.

    Args:
        url (str): The URL to be processed.
        force (bool): A flag indicating whether to force the addition of the URL even if it already exists.

    Returns:
        None
    """
    logging.debug(f"Processing URL: {url}")
    resolved_url = resolve_redirect(url)  # Resolve the redirect
    logging.debug(f"Resolved URL: {resolved_url}")
    
    current_time = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')  # Format without milliseconds
    
    if "youtube.com" in resolved_url and not resolved_url.startswith('https://www.youtube.com/redirect'):
        try:
            result = get_youtube_data(resolved_url)
            if isinstance(result, tuple) and isinstance(result[2], list):  # Playlist
                playlist_title, playlist_uploader, videos = result
                playlist_url = resolved_url
                if not url_exists_in_file(playlist_url, UNIFIED) or force:
                    append_to_file(UNIFIED, f"{current_time}|[{playlist_url}]|({playlist_title})|{playlist_uploader}|YouTube\n")
                    print(f"{current_time}|[{playlist_url}]|({playlist_title})|{playlist_uploader}|YouTube")
                    logging.info(f"Added playlist URL: {playlist_url}")
                for video_id, title, uploader in videos:
                    title = re.sub('[^0-9a-zA-Z]+', ' ', title).strip()
                    video_url = f"https://www.youtube.com/watch?v={video_id}"
                    if not url_exists_in_file(video_url, UNIFIED) or force:
                        append_to_file(UNIFIED, f"{current_time}|[{video_url}]|({title})|{uploader}|YouTube\n")
                        print(f"{current_time}|[{video_url}]|({title})|{uploader}|YouTube")
                        logging.info(f"Added video URL: {video_url}")
                    else:
                        print(f"Error: URL {video_url} already recorded.")
                        logging.warning(f"Duplicate URL: {video_url}")
            else:  # Single Video
                video_id, title, uploader = result
                title = re.sub('[^0-9a-zA-Z]+', ' ', title).strip()
                video_url = f"https://www.youtube.com/watch?v={video_id}"
                if not url_exists_in_file(video_url, UNIFIED) or force:
                    append_to_file(UNIFIED, f"{current_time}|[{video_url}]|({title})|{uploader}|YouTube\n")
                    print(f"{current_time}|[{video_url}]|({title})|{uploader}|YouTube")
                    logging.info(f"Added video URL: {video_url}")
                else:
                    print(f"Error: URL {video_url} already recorded.")
                    logging.warning(f"Duplicate URL: {video_url}")
        except ValueError as e:
            print(f"Error: {e}")
            logging.error(f"Invalid YouTube URL: {e}")
    else:
        title = get_title_from_url(resolved_url)
        logging.debug(f"Fetched title: {title}")
        if title == "Dead link":
            print(f"Error: The URL {resolved_url} is a dead link.")
            logging.error(f"Dead link detected: {resolved_url}")
        elif title == "Timeout error":
            print(f"Error: The request to {resolved_url} timed out.")
            logging.error(f"Timeout error detected: {resolved_url}")
        elif title == "Too many redirects":
            print(f"Error: The URL {resolved_url} has too many redirects.")
            logging.error(f"Too many redirects detected: {resolved_url}")
        elif title.startswith("Unexpected error"):
            print("Error: An unexpected error occurred.")
            logging.error(f"Unexpected error with URL: {resolved_url}")
        elif title and not title.startswith("Error"):
            if url_exists_in_file(resolved_url, UNIFIED) and not force:
                print(f"Error: URL {resolved_url} already recorded.")
                logging.warning(f"Duplicate URL: {resolved_url}")
            else:
                append_to_file(UNIFIED, f"{current_time}|[{resolved_url}]|({title})|General|General\n")
                print(f"{current_time}|[{resolved_url}]|({title})|General|General")
                logging.info(f"Added URL: {resolved_url}")
        else:
            print("Invalid URL")
            logging.error(f"Invalid URL: {resolved_url} with title: {title}")


def create_backup(file_path: str) -> None:
    """
    Creates a backup of the specified file.
    
    Args:
        file_path (str): The path to the file that needs to be backed up.
    
    Returns:
        None
    """
    timestamp = datetime.now().strftime("%Y%m%dT%H%M%S")
    backup_file_path = f"{os.path.dirname(file_path)}/{timestamp}_{os.path.basename(file_path)}"
    try:
        with open(file_path, 'r') as original_file:
            with open(backup_file_path, 'w') as backup_file:
                backup_file.write(original_file.read())
        print(f"Backup created: {backup_file_path}")
        logging.info(f"Backup created: {backup_file_path}")
    except Exception as e:
        print(f"Error creating backup: {e}")
        logging.error(f"Error creating backup: {e}")

def main() -> None:
    """
    Main function to handle the command-line interface for recording URLs.
    """
    ensure_path_exists(UNIFIED)
    try:
        args = parse_arguments()

        # Set logging level based on the debug argument
        if args.debug == 1:
            logging.getLogger().setLevel(logging.INFO)
        elif args.debug == 2:
            logging.getLogger().setLevel(logging.WARNING)
        elif args.debug == 3:
            logging.getLogger().setLevel(logging.DEBUG)
        else:
            logging.getLogger().setLevel(logging.ERROR)

        if args.integrity:
            integrity_errors = check_integrity()
            if integrity_errors:
                print("Integrity check failed:")
                for error in integrity_errors:
                    file_name, line_number, line_contents, expected_line = error
                    print(f"{file_name} line {line_number}: {line_contents}\nExpected: {expected_line}")
            else:
                print("Integrity check passed. Log files are formatted correctly.")
        elif args.backup:
            create_backup(UNIFIED)
        elif args.search:
            search_term = args.search
            all_fields = ["url", "title", "date", "source", "uploader"]
            results = {}
            for field in all_fields:
                field_results = search_entries(search_term, field, UNIFIED)
                for line, hit_types in field_results.items():
                    if line not in results:
                        results[line] = hit_types
                    else:
                        results[line].extend(hit_types)
            for line, hit_types in results.items():
                unique_hit_types = list(set(hit_types))
                print(line.strip())
                for hit_type in unique_hit_types:
                    print(f"-Hit Type: {hit_type}")
        elif args.search_url:
            results = search_entries(args.search_url, "url", UNIFIED)
            for line, hit_types in results.items():
                unique_hit_types = list(set(hit_types))
                print(line.strip())
                for hit_type in unique_hit_types:
                    print(f"-Hit Type: {hit_type}")
        elif args.search_title:
            results = search_entries(args.search_title, "title", UNIFIED)
            for line, hit_types in results.items():
                unique_hit_types = list(set(hit_types))
                print(line.strip())
                for hit_type in unique_hit_types:
                    print(f"-Hit Type: {hit_type}")
        elif args.search_date:
            results = search_entries(args.search_date, "date", UNIFIED)
            for line, hit_types in results.items():
                unique_hit_types = list(set(hit_types))
                print(line.strip())
                for hit_type in unique_hit_types:
                    print(f"-Hit Type: {hit_type}")
        elif args.search_source:
            results = search_entries(args.search_source, "source", UNIFIED)
            for line, hit_types in results.items():
                unique_hit_types = list(set(hit_types))
                print(line.strip())
                for hit_type in unique_hit_types:
                    print(f"-Hit Type: {hit_type}")
        elif args.search_uploader:
            results = search_entries(args.search_uploader, "uploader", UNIFIED)
            for line, hit_types in results.items():
                unique_hit_types = list(set(hit_types))
                print(line.strip())
                for hit_type in unique_hit_types:
                    print(f"-Hit Type: {hit_type}")
        elif args.url:
            process_url(args.url, args.force)
        else:
            while True:
                try:
                    url = input("Enter a URL to record (or press Ctrl+C to quit): ")
                    force = False
                    process_url(url, force)
                except Exception as e:
                    print(f"An error occurred: {e}")
                    logging.error(f"An error occurred: {e}")
    except KeyboardInterrupt:
        print("\nExiting...")
        sys.exit(0)

if __name__ == "__main__":
    if DEVELOPER_KEY is None:
        print('Error: YOUTUBE_API_KEY is not set in .env file or shell environment.')
        choice = input('Would you like to set it now? (yes/no): ')
        if choice.lower() == 'yes':
            set_developer_key()
    else:
        if os.getenv('DEBUG') == 'True':
            print('YOUTUBE_API_KEY is set.')
    main()

