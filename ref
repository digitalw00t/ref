#!/usr/bin/python3

import os
import re
import sys
import argparse
from urllib.parse import urlparse
from datetime import date
import requests
from bs4 import BeautifulSoup

BASE = "/home/falgout"
YOUTUBE = os.path.join(BASE, "references", "youtube_references.md")
LINKS = os.path.join(BASE, "references", "links.md")

def parse_arguments():
    parser = argparse.ArgumentParser(description="Add URL to markdown files.")
    parser.add_argument("url", nargs='?', default=None, help="URL to be added.")
    parser.add_argument("-f", "--force", action="store_true", help="Force addition even if URL already exists.")
    parser.add_argument("-e", "--edit", action="store_true", help="Open markdown file for editing.")
    args = parser.parse_args()

    if args.url:
        args.url = args.url.replace('&', '\&')

        if args.edit:
            if "www.youtube.com" in args.url:
                os.system(f"vim {YOUTUBE}")
                sys.exit()
            else:
                os.system(f"vim {LINKS}")
                sys.exit()
    return args

def url_exists_in_file(url, file_path):
    with open(file_path, "r") as f:
        for line in f:
            # Use regular expression to find URLs in the line
            urls = re.findall(r'\[([^\]]+)\]', line)
            for url_str in urls:
                # Extract the URL from the Markdown link format [URL]
                extracted_url = url_str.split(']')[0]
                if url == extracted_url:
                    return True
    return False

def get_channel_name(url):
    parsed_url = urlparse(url)
    channel_path = parsed_url.path.rstrip('/')
    channel_name = channel_path.split('/')[-1]
    return channel_name

def get_title_from_url(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            title = soup.title.string.strip() if soup.title and soup.title.string else "No title found"
            return title
        else:
            return f"Failed to fetch page content (status code: {response.status_code})"
    except Exception as e:
        return f"Error fetching title: {str(e)}"

def process_url(url, force):
    if "youtube.com" in url:
        channel_id = re.search(r"/(user|channel)/([\w-]+)", url)
        
        if channel_id:
            channel_id = channel_id.group(2)
            channel_url = f"https://www.youtube.com/{channel_id}"
            if not force and url_exists_in_file(channel_url, YOUTUBE):
                print(f"Error: Channel URL {channel_url} already recorded.")
            else:
                channel_name = get_channel_name(channel_url)
                if channel_name:
                    with open(YOUTUBE, "a") as f:
                        f.write(f"{date.today()}|[{channel_url}]|Youtube Creator {channel_name}\n")
                    print(f"{date.today()}|[{channel_url}]|Youtube Creator {channel_name}")
                else:
                    print("Invalid YouTube Channel URL")
            
        else:
            video_id, title, uploader = get_youtube_data(url)
            if video_id and title and uploader:
                title = re.sub('[^0-9a-zA-Z]+', ' ', title)
                title = " ".join(title.split())
                video_url = f"https://www.youtube.com/watch?v={video_id}"
                if not force and url_exists_in_file(video_url, YOUTUBE):
                    print(f"Error: Video URL {video_url} already recorded.")
                else:
                    with open(YOUTUBE, "a") as f:
                        f.write(f"{date.today()}|[{video_url}]|({title})|{uploader}\n")
                    print(f"{date.today()}|[{video_url}]|({title})|{uploader}")
            else:
                print("Invalid YouTube URL")

    else:
        title = get_title_from_url(url)
        if title and not title.startswith("Error"):
            if not force and url_exists_in_file(url, LINKS):
                print(f"Error: URL {url} already recorded.")
            else:
                with open(LINKS, "a") as f:
                    f.write(f"{date.today()}|[{url}]|({title})\n")
                print(f"{date.today()}|[{url}]|({title})")
        else:
            print("Invalid URL")

def main():
    try:
        args = parse_arguments()
        if args.url:
            process_url(args.url, args.force)
        else:
            while True:
                try:
                    url = input("Enter a URL to record (or press Ctrl+C to quit): ")
                    force = input("Force addition even if URL already exists? (y/n): ").lower() == 'y'
                    process_url(url, force)
                except Exception as e:
                    print(f"An error occurred: {e}")
    except KeyboardInterrupt:
        print("\nExiting...")
        sys.exit(0)

if __name__ == "__main__":
    main()

